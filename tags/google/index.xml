<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>google on baishen</title><link>https://baishen.me/tags/google/</link><description>Recent content in google on baishen</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Wed, 04 Sep 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://baishen.me/tags/google/index.xml" rel="self" type="application/rss+xml"/><item><title>MegaStore笔记</title><link>https://baishen.me/blog/2019-09-04-megastore-note/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://baishen.me/blog/2019-09-04-megastore-note/</guid><description>Megastore 融合了 NoSQL 的扩展性(scalability)和 RDBMS 的便利性(convenience)， 提供强一致性保证(strong consistency guarantees) 和高可用性(high availability)。 在数据的细粒度分区(fine-grained partitions)中提供了完全可序列化的 ACID 语义。 在广域网同步复制(synchronously replicate)每一次写操作，具有合理的延迟(reasonable latency)， 支持在数据中心间无缝故障转移(seamless failover)。
1 介绍(INTRODUCTION) 在线业务上云给存储带来的挑战：
高可扩展(highly scalable) 快速开发(rapid development) 低延迟(low latency) 数据的一致视图(consistent view of the data) 高可用(highly available) 现状：
RDBMS 扩展性差 NoSQL 开发困难（有限的 API，松散一致性模型） Megastore 融合 RDBMS 和 NoSQL 的优势。 通过同步复制(synchronous replication) 达到高可用(high availability) 和数据的一致视图(consistent view of the data)。
对数据分区并分别复制，分区内提供 ACID 语义，分区间提供有限的一致性保证 提供部分数据库功能，例如二级索引 使用 Paxos 复制数据（首创） 2 提供可用性和扩展性(TOWARD AVAILABILITY AND SCALE) 设计目标：</description></item><item><title>Percolator笔记</title><link>https://baishen.me/blog/2019-08-05-percolator-note/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://baishen.me/blog/2019-08-05-percolator-note/</guid><description>Google 的索引系统存储数十 PB 的数据，每天有数十亿的更新。 MapReduce 适合处理大批量的数据，处理少量更新效率较低。 Percolator 为处理增量更新而设计，处理同样数据量的文档时可以将平均时延降低 50%。
1 介绍(Introduction) 爬虫抓取页面之后需要一系列 MapReduce 操作来构建索引。 包括聚类排重（处理相同页面的内容，显示 PageRank 高的），构建倒排。 MapReduce 限制并行计算，计算完 PageRank 对应的 url 之后再构建倒排，不用担心 PageRank 变化。
少量页面变化时构建索引需要把全部页面处理一遍，导致更新速度过慢。 索引可以存储在 DBMS 中，通过事务更新单行数据，但是 DBMS 无法支持 Google 的数据量（分布在数千台机器的数十PB数据）。 分布式的 Bigtable 可以支持这个数据量，但是不支持跨行事务，无法保证并发更新时数据的不变性。
为增量索引更新优化的数据处理系统应该满足如下要求：
可以维护一个巨大的文档仓库，同时在爬取到新页面的时候可以高效的更新 并发处理很多小的更新时需要维持不变性（事务） 跟踪哪些更新被处理了 Percolator 被设计用来解决这个问题，具有如下特点：
随机访问数十 PB 数据的仓库（避免全局扫描） 多台机器上的多个进程并发访问数据（高吞吐） 提供 ACID 事务支持（快照隔离级别 snapshot isolation） 提供观察者（observers），跟踪增量计算的状态 系统由一组观察者组成，级联处理数据，第一个观察者由外部进程触发 Percolator 专为处理增量数据设计，不适合如下场景：
不能划分为小更新的计算（排序）更适合用 MapReduce 计算没有强一致性要求，直接使用 BigTable 资源需求较少（数据量小、CPU需求少等），直接使用传统 DBMS Google 内部使用 Percolator 构建网页索引。 在爬虫运行的同时处理文档，可以将平均时延减少50%。 通过跟踪网页与它依赖的资源，在任意依赖发生变化的时候对页面重新进行处理。</description></item><item><title>Big Table笔记</title><link>https://baishen.me/blog/2019-07-22-bigtable-note/</link><pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate><guid>https://baishen.me/blog/2019-07-22-bigtable-note/</guid><description>Bigtable 是一个分布式存储系统，用于管理超大规模的结构化数据
Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers
1 介绍(Introduction) PB级数据 数千台机器 适用性广泛(wide applicability) 可扩展(scalability) 高性能(high performance) 高可用(high availability) 类似数据库 提供简单的数据模型 数据都视为字符串 2 数据模型(Data Model) Bigtable 是一个稀疏的、分布式的、持久化存储的多维度排序 Map Map 通过行、列、时间戳索引 Map 中的每个 value 都是一个未解析的 byte 数组 A Bigtable is a sparse, distributed, persistent multi- dimensional sorted map.</description></item><item><title>Map Reduce笔记</title><link>https://baishen.me/blog/2019-07-13-map-reduce-note/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>https://baishen.me/blog/2019-07-13-map-reduce-note/</guid><description>1 介绍(Introduction) 处理原始数据
文档抓取 Web 请求日志 处理衍生数据
倒排索引 Web 文档的图结构(graph structure) 表示 每台主机爬虫抓取数量汇总 每天被请求最多的的查询的集合 难点
并行计算 分发数据 处理错误 大多数运算包含相同的操作
输入数据应用 Map 得到一个 key/value pair 集合 在相同 key 值的 value 上应用 Reduce 操作，合并数据，得到结果 MapReduce 框架模型
处理并行计算、容错、数据分布、负载均衡 通过简单的接口来实现自动的并行化和大规模的分布式计算 2 编程模型(Programming Model) 利用一个输入 key/value pair 集合来产生一个输出的 key/value pair 集合
用户自定义的 Map 函数接收 key/value pair，输出 key/value pair 集合 MapReduce 库把所有相同 key 的 value 集合后传递给 Reduce 函数 用户自定义的 Reduce 函数合并 value 值，形成较小的 value 值集合 2.</description></item><item><title>GFS笔记</title><link>https://baishen.me/blog/2019-07-10-gfs-note/</link><pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate><guid>https://baishen.me/blog/2019-07-10-gfs-note/</guid><description>1 介绍(INTRODUCTION) 规模：数千台机器、数百T存储空间、数百个客户机 设计目标：性能、可伸缩性、可靠性、可用性 组件失效是常态事件 文件非常巨大 大部分修改是在文件尾部追加数据 应用程序和文件系统 API 协同设计 2 设计概述(DESIGN OVERVIEW) 2.1 设计预期(Assumptions) 组件失效是常态事件，必须有持续的监控、错误侦测、灾难冗余以及自动恢复的机制 主要存储大文件（数GB），不需要针对小文件优化 读操作 大规模的流式读取 小规模的随机读取 写操作 大规模的、顺序的、数据追加方式的写，数据一旦写入就很少修改 支持小规模的随机写，但是效率不高 支持多个客户端并行追加数据到同一个文件，文件通常使用生产者-消费者队列或多路合并 持续的高带宽比低延迟更重要 2.2 接口(Interface) 类似传统文件系统的接口，但是不严格按照POSIX实现 支持常见的文件操作，create、delete、open、close、read、write 还支持 snapshot 和 record append 2.3 架构(Architecture) 集群包含一个 Master、多个 Chunkservers，同时被多个 Clients 访问 运行在 Linux 的 user-level 进程 文件被分割成固定大小的 Chunks，创建 Chunk 时，Master 分配一个全局唯一 (globally unique)不变(immutable)的64位标识 为了可靠性，每个 Chunk 会被复制到多个 Chunkservers，默认3个 Master管理所有的文件的 metadata 命名空间(namespace) 访问控制信息(access control information) 文件到Chunk的映射(mapping from files to Chunks) Chunk的当前位置(current locations of Chunks) Chunk租约管理(Chunk lease management) 孤儿Chunk的回收(garbage collection of orphaned Chunks) Chunk 在 Chunkserver间的迁移(Chunkmigration between Chunkservers) Master 与各个 Chunkserver通过心跳(HeartBeat)周期性的通信，传输命令并收集 Chunkserver 状态信息 Client 以库的形式嵌入到客户程序中。Clients 和 Master 通信获取 metadata，数据直接从 Chunkserver 获取 Client 和 Chunkserver 都不缓存文件数据(Client 缓存 metadata) 2.</description></item></channel></rss>