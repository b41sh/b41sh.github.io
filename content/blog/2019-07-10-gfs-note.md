---
title: "GFS笔记"
date: '2019-07-10'
tags:
  - papers
  - google
---

## 1 介绍(INTRODUCTION)

- 规模：数千台机器、数百T存储空间、数百个客户机
- 设计目标：性能、可伸缩性、可靠性、可用性

1. 组件失效是常态事件
2. 文件非常巨大
3. 大部分修改是在文件尾部追加数据
4. 应用程序和文件系统 API 协同设计

## 2 设计概述(DESIGN OVERVIEW)

### 2.1 设计预期(Assumptions)

1. 组件失效是常态事件，必须有持续的监控、错误侦测、灾难冗余以及自动恢复的机制
2. 主要存储大文件（数GB），不需要针对小文件优化
3. 读操作
  - 大规模的流式读取
  - 小规模的随机读取
4. 写操作
  - 大规模的、顺序的、数据追加方式的写，数据一旦写入就很少修改
  - 支持小规模的随机写，但是效率不高
5. 支持多个客户端并行追加数据到同一个文件，文件通常使用生产者-消费者队列或多路合并
6. 持续的高带宽比低延迟更重要

### 2.2 接口(Interface)

- 类似传统文件系统的接口，但是不严格按照POSIX实现
- 支持常见的文件操作，create、delete、open、close、read、write
- 还支持 snapshot 和 record append

### 2.3 架构(Architecture)

- 集群包含一个 Master、多个 Chunkservers，同时被多个 Clients 访问
- 运行在 Linux 的 user-level 进程
- 文件被分割成固定大小的 Chunks，创建 Chunk 时，Master 分配一个全局唯一
(globally unique)不变(immutable)的64位标识
- 为了可靠性，每个 Chunk 会被复制到多个 Chunkservers，默认3个
- Master管理所有的文件的 metadata
  - 命名空间(namespace)
  - 访问控制信息(access control information)
  - 文件到Chunk的映射(mapping from files to Chunks)
  - Chunk的当前位置(current locations of Chunks)
  - Chunk租约管理(Chunk lease management)
  - 孤儿Chunk的回收(garbage collection of orphaned Chunks)
  - Chunk 在 Chunkserver间的迁移(Chunkmigration between Chunkservers)
- Master 与各个 Chunkserver通过心跳(HeartBeat)周期性的通信，传输命令并收集 Chunkserver 状态信息
- Client 以库的形式嵌入到客户程序中。Clients 和 Master 通信获取 metadata，数据直接从 Chunkserver 获取
- Client 和 Chunkserver 都不缓存文件数据(Client 缓存 metadata)

### 2.4 单一 Master(Single Master)

- 单一 Master 大大简化了设计
- Clients 缓存 metadata，减少对 Master 的读操作，避免 Master 成为系统瓶颈

Client读流程

1. 利用固定大小的 Chunk size，Client 把文件名(file name)和偏移量(byte offset)翻译成 Chunk index
2. 向 Master 发送包含 file name 和 Chunk index 的请求(通常一次请求多个 Chunk 信息)
3. Master 返回对应的 Chunk handle 和副本(replicas)的位置
4. Client 以 file name 和 Chunk index 为 key 缓存这些信息
5. Client 向一个副本发送读请求(通常是最近的一个)，指定 Chunk handle 和字节范围(byte range)

### 2.5 Chunk 尺寸(Chunk Size)

关键设计参数之一，选择64MB（远大于一般文件系统的 block size）

大 Chunk size 的优势

1. 减少 Client 和 Master 的交互次数
2. Client 可以对一个 Chunk 进行多次操作，通过建立一个持久的 TCP 连接减少网络负载
3. 减少 Master 存储的 metadata size，这样可以把 metadata 存储到内存中

大 Chunk size 的劣势

小文件 Chunk 少，多个 Client 同时读取的时候会形成热点(hot spots)

### 2.6 元数据(Metadata)

Master 存储三种主要类型的 metadata

1. 命名空间(file and Chunk namespace)
2. 文件到Chunk的映射(mapping from files to Chunks)
3. 各个Chunk副本的位置(locations of each Chunk’s replicas)

- 所有的 metadata 都存储在 Master 的内存中
- 1和2通过日志持久化并复制到远程机器，用于故障时恢复
- 3不持久化，通过轮询 Chunkserver 获取

#### 2.6.1 内存中的数据结构(In-Memory Data Structures)

- Master 操作非常快
- 简洁性、可靠性、高性能和灵活性
- Master 可以在后台高效地周期性遍历 metadata 的全部状态，完成如下操作：

1. Chunk gc(garbage collection)
2. Chunkserver 失效时重新复制数据
3. 在 Chunkserver 间迁移 Chunk 以平衡负载和磁盘空间使用率

潜在问题：Chunk 数量受限于 Master 内存大小

- 64 字节可以管理一个 64MB 的 Chunk
- 大多数文件包含多个Chunk，除最后一个文件部分填充外，其它文件都是满的
- 增加内存很容易

#### 2.6.2 Chunk 位置(Chunk Locations)

- Master 启动时轮询 Chunkserver 获取 Chunk 位置
- 周期性心跳监控 Chunkserver 状态
- 简化 Chunkserver 加入集群、离开集群、更名、实效、重启的时候，Master与Chunkserver数据同步的问题

#### 2.6.3 操作日志(Operation Log)

- 日志包含了关键的 metadata 变化历史记录
- 日志复制到多台远程机器，确保日志文件完整
- 日志记录持久化成功(本机和远程机器)，才会返回客户端
- 日志达到一定量时，做一次 checkpoint(B-树存储)
- 通过 checkpoint 和后续的日志恢复到最近的状态

### 2.7 一致性模型(Consistency Models)

- 宽松的一致性模型(relaxed consistency model)
- 相对简单、容易实现

#### 2.7.1 GFS一致性保障(Guarantees by GFS)

- 文件命令空间的修改是原子性的
- 命名空间锁提供了原子性和正确性保障，仅由 Master 控制
- Master 操作日志定义这些操作的全局顺序

region 的状态取决于操作的类型、成功与否、以及是否同步修改

- 一致：所有客户端无论从哪个副本读取，读到的数据都一样
- 已定义：一致	，并且客户端能够看到写入操作全部的内容（没有受到同时执行的其它写入操作的干扰）
- 一致未定义：所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的数据
（包含了来自多个修改操作的、混杂的数据片段）
- 不一致：不同的客户在不同的时间会看到不同的数据（失败的写入操作）（填充数据或重复数据）

GFS 确保修改的文件是已定义的

1. 对 Chunk 所有副本修改操作顺序一致
2. 使用 Chunk 版本号检测副本是否错过修改导致失败，通过 gc 回收

Client 缓存 Chunk 位置，可能会从失效的副本读到数据

Master 定期和 Chunkserver 通信，检测数据是否损坏，通过有效副本恢复

#### 2.7.2 程序的实现(Implications for Applications)

- 尽量采用追加写入而不是覆盖
- Checkpoint
- 自验证的写入操作
- 自标识的记录（包含 checksum 验证有效性）

## 3 系统交互(SYSTEM INTERACTIONS)

重要的原则：最小化所有操作和 Master 节点的交互

### 3.1 租约和变更顺序(Leases and Mutation Order)

- 租约(lease)机制保持多个副本间变更顺序的一致性
- 最小化 Master 的管理负担
- Master 为 Chunk 的一个副本建立租约，称为主 Chunk

写入流程

1. Client 向 Master 询问持有 Chunk 租约的 Chunkserver，Chunk 其它副本的位置。
如果没有，Master 选择 Chunk 的一个副本建立租约
2. Master 将主 Chunkserver 的标识和其它副本的位置返回给 Client，Client 进行缓存
3. Client 将数据推送到所有副本。Chunkserver 将数据保存在内部的 LRU 缓存
4. 一旦所有副本回复已经收到数据，Client 向主副本发送写请求。主副本向接收到的所有操作
分配连续的序列号，操作可能来自不同客户端
5. 主副本把写请求发送给所有二级副本，二级副本按照主副本分配的序列号以相同的顺序执行
6. 二级副本向主副本回复完成操作
7. 主副本回复 Client。任何副本的错误都会返回，Client 重新执行 3-7 的操作

### 3.2 数据流(Data Flow)

- 提高网络效率：数据流和控制流分开的措施
- 充分利用每台机器的带宽：数据沿着 Chunkserver 链的顺序推送
- 尽可能的避免出现网络瓶颈和高延迟的链接：尽量选择没有接收数据，离自己最近的机器推送
- 最小化延迟：利用基于 TCP 连接的、管道式的数据推送方式

### 3.3 原子的记录追加(Atomic Record Appends)

- 提供原子的数据追加操作：记录追加
- 保证至少有一次原子的写入操作成功执行
- 记录追加是修改操作
- 有副本失败需要重新推送
  - 同一个 Chunk 的不同副本可能包含不同的数据
  - 不保证所有副本在字节级别一致

### 3.4 快照(Snapshot)

1. 取消快照文件所有 Chunk 的租约
2. 租约过期后，把操作日志记录到硬盘
3. 复制源文件或目录元数据，把日志条目的变化保存到内存中
4. Client 第一次写数据到 Chunk 的时候，发现 Chunk 引用计数大于一，克隆 Chunk

## 4 Master 节点的操作(MASTER OPERATION)

- 执行所有的名称空间操作
- 管理整个系统里所有 Chunk 的副本
  - 决定 Chunk 的存储位置
  - 创建新 Chunk 和它的副本
  - 协调各种各样的系统活动以保证 Chunk 被完全复制
  - 在所有的 Chunk 服务器之间的进行负载均衡
  - 回收不再使用的存储空间

### 4.1 名称空间管理和锁(Namespace Management and Locking)

- 不能列出目录下所有的文件
- 不支持文件路径或目录的链接
- 名称空间是一个全路径和元数据映射关系的查找表
- 操作前要获取锁，父目录的读锁和当前文件/目录的读写锁
- 使用锁可以支持同一目录的并行操作

### 4.2 副本的位置(Replica Placement)

两大目标：

- 最大化数据可靠性和可用性(副本分布在多个机架)
- 最大化网络带宽利用率

### 4.3 创建、重新创建、重新负载均衡(Creation, Re-replication, Rebalancing)

创建 Chunk 时的考虑因素：

1. 低于平均硬盘使用率的 Chunkserver
2. 限制每个 Chunkserver 上最近的 Chunk 创建次数
3. Chunk 分布在多个机架

重新创建：副本数量少于用户指定的复制因数

- 优先复制数量少的 Chunk
- 优先复制活跃的 Chunk
- 优先复制阻塞 Client 流程的 Chunk

周期性的重新负载均衡，移动副本以更好的利用硬盘

### 4.4 垃圾回收(Garbage Collection)

惰性回收，

#### 4.4.1 机制(Mechanism)

文件删除的流程

1. 记录删除操作的日志
2. 将文件改名为一个包含删除时间戳、隐藏的名字
3. Master 对命名空间常规扫描时，删除3天前的隐藏文件
4. 删除前可以用特殊方式读取，可以反删除

常规扫描时会发现孤儿 Chunk （不被任何文件包含）

#### 4.4.2 讨论(Discussion)

分布式垃圾回收是一个复杂的问题

GFS 可以很容易的处理

- Chunk 的所有引用：存储在 Master 的文件到块的映射表中
- 所有 Chunk 的副本：以 Linux 文件的形式存储在 Chunk 服务器的指定目录下
- Master 不能识别的副本是垃圾副本

优势：

1. 简单可靠（消息可能会丢失，可以不断重试）
2. 垃圾回收合并到 Master 后台活动中，分散开销
3. 延迟删除更安全

缺点：

- 阻碍用户调优存储空间的使用
- 存储空间不能马上释放（可以显式删除加快回收）
- 不同命名空间可以制定不同的复制和回收策略

### 4.5 过期失效的副本检测(Stale Replica Detection)

- Chunkserver 失效时，Chunk 副本会错失修改而过期失效
- Master 通过 Chunk 的版本号区分当前副本和过期副本
- Master 与 Chunk 签订租约时增加版本号，然后通知副本(Replica)
- Master 在垃圾回收的过程中移除过期失效的版本

## 5 容错和诊断(FAULT TOLERANCE AND DIAGNOSIS)

最大挑战之一：如何处理频繁发生的组件失效

### 5.1 高可用性(High Availability)

高可用性保证：

- 快速恢复
- 复制

#### 5.1.1 快速恢复(Fast Recovery)

- 数秒内恢复
- 不区分正常关闭和异常关闭

#### 5.1.2 Chunk复制(Chunk Replication)

- 每个 Chunk 都被复制到不同机架上不同的 Chunkserver
- 为不同的命名空间设置不同的复制级别，默认3个
- 其它冗余方案
  - 奇偶校验
  - Erasure codes

#### 5.1.3 Master复制(Master Replication)

- Master 的所有操作日志和 Checkpoint 都被复制到多台机器
- 操作日志写入到本机和备机，修改操作才能提交成功
- Master 失效时，监控程序在其它存有完整日志的备机中启动新的 Master 进程
- Client 通过规范的名字访问 Master，类似 DNS
- 影子 Master 在主 Master 宕机时提供只读服务
- 影子 Master 更新比主 Master 慢，通常不到1秒

### 5.2 数据完整性(Data Integrity)

- Chunkserver 使用 Checksum 来检查数据完整性
- 每个 Chunk 分为 64KB 的 block，每个 block 对应一个 32位的 Checksum
- 读操作时检查 Checksum 是否正确
  - 返回 Client 错误信息
  - 通知 Master，从其它副本恢复数据
- Checksum 效验不需要额外 IO，对性能影响小
- Checksum 对 Chunk 追加写入操作做了优化，只增量更新最后一个不完整 block 的 Checksum
- Chunkserver 空闲时扫描和校验不活动的 Chunk

### 5.3 诊断工具(Diagnostic Tools)

- 保留详细的日志有利于问题隔离、调试、性能分析
- RPC 日志包含所有网络请求和响应的记录
- 日志可用来跟踪负载测试和性能分析

## 6 度量(MEASUREMENTS)

### 6.1 小规模基准测试(Micro-benchmarks)

1 台 Master ，2 台 Master 复制节点，16 台 Chunkserver 和 16 个 Client

- 读取：理论值的 75%，多个 Client 读同一个 Chunkserver
- 写入：理论值的 50%，需要写3个副本，并行写入冲突较大
- 记录追加：性能受限于最后一个 Chunk 的 Chunkserver，与 Client 数量无关

### 6.2 实际应用中的集群(Real World Clusters)

- 上百台 Chunkserver，数 TB 硬盘
- Chunkserver 保存十几 GB 元数据，主要是 Checksum、Chunk 的版本号
- Master 保存几十 MB 元数据
- 每台服务器保存 50MB 到 100MB 元数据，恢复速度非常快
- 读的速率高于写的速率
- 发送到 Master 的请求是每秒钟 200 到 500，名字空间二分查找进行搜索

### 6.3 工作负荷分析(Workload Breakdown)

- 读写操作按数据量大小也同样呈现为双峰分布
- 记录追加操作的数量多于写操作

## 7 经验(EXPERIENCES)

- 起初定位是后端文件系统，逐步增加对开发的支持（权限、配额功能）
- 最大的问题：磁盘、Linux
  - 磁盘驱动
  - Linux2.2 fsync效率问题
  - 读写锁

## 8 相关工作(RELATED WORK)

- 提供了一个与位置无关的名字空间，类似[AFS]
- 文件存储到不同服务器，类似[Xfs]、[Swift]
- 复制比 RAID 简单
- 文件层面不提供 Cache，与 [AFS]、[xFS]、[Frangipani]、[Intermezzo] 不同
- 使用中心服务器，与 [Frangipani]、[xFS]、[Minnesota’s GFS]、[GPFS] 不同
- 有大量客户端时保障系统整体性能，与 [Lustre] 相同
- 灾难冗余方案是设计的核心
- 架构类似 [NASD]，实现重新负载均衡、复制、恢复机制
- 不改变存储设备的 Model，不同于与 [Minnesota’s GFS] 和 [NASD]
- 通过原子的记录追加操作实现了生产者-消费者队列，类似 [River] 的分布式队列

## 9 结论(CONCLUSIONS)

使用普通硬件支持大规模数据处理的系统

根据当前和可预期将来的应用规模和技术环境来评估传统的文件系统的特性

- 组件失效是常态而不是异常
- 采用追加方式(有可能是并发追加)写入
- 通常序列化读取大文件
- 扩展标准文件系统接口、放松接口限制

通过持续监控，复制关键数据，快速和自动恢复提供灾难冗余

保证在有大量的并发读写操作时能够提供很高的合计吞吐量（分离控制流和数据流）

## 参考资料

1. [The Google File System]
2. [GFS中文版]
3. [6.824-notes]
4. [GFS案例学习]

<!-- links -->

[The Google File System]: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf
[GFS中文版]: http://blog.bizcloudsoft.com/wp-content/uploads/Google-File-System%E4%B8%AD%E6%96%87%E7%89%88_1.0.pdf
[6.824-notes]: https://pdos.csail.mit.edu/6.824/notes/l-gfs-short.txt
[GFS案例学习]: https://github.com/feixiao/Distributed-Systems/blob/master/Lec03_GFS/GFS.md

[AFS]: https://www.cs.cmu.edu/~satya/docdir/howard-tocs-afs-1988.pdf
[Xfs]: https://www.cs.utexas.edu/users/less/publications/research/xFS.tocs96.pdf
[Swift]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6767&rep=rep1&type=pdf
[Frangipani]: https://pdos.csail.mit.edu/6.824/papers/thekkath-frangipani.pdf
[Intermezzo]: https://www.inter-mezzo.org/
[Minnesota’s GFS]: https://www.lcse.umn.edu/projects/Paul/soltis96global.pdf
[GPFS]: https://www.usenix.org/legacy/events/fast02/full_papers/schmuck/schmuck.pdf
[Lustre]:http://lustre.org/
[NASD]: https://www.pdl.cmu.edu/PDL-FTP/NASD/asplos98.pdf
[River]: http://now.cs.berkeley.edu/files/recent/river.pdf

